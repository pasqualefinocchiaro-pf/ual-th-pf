{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d205308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54535ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ± VERTICAL FARM MESSAGE ANALYZER\n",
      "============================================================\n",
      "\n",
      "1. Use sample data\n",
      "2. Load from Google Drive\n",
      "\n",
      "ðŸ“Š Processing messages...\n",
      "Found 5 messages\n",
      "\n",
      "âœ… Saved 5 results to analysis_results.csv\n",
      "âœ… Saved 4 SQL queries to queries.sql\n",
      "\n",
      "ðŸ“ˆ Summary:\n",
      "Users: {'gianni': 3, 'maria': 2}\n",
      "Request types: {'exclusion': 3, 'analysis': 1, 'general': 1}\n",
      "\n",
      "âœ… Complete! Check the generated files.\n",
      "\n",
      "Sample results:\n",
      "             timestamp    user       type  \\\n",
      "0  2025-07-18 12:43:08  gianni  exclusion   \n",
      "1  2025-04-07 06:51:35   maria   analysis   \n",
      "2  2025-03-06 07:29:37   maria    general   \n",
      "3  2025-02-19 13:33:59  gianni  exclusion   \n",
      "4  2025-06-20 12:34:42  gianni  exclusion   \n",
      "\n",
      "                                             message  \\\n",
      "0  Eccoti i dati da escludere questa settimana: A...   \n",
      "1  riusciresti a dirmi qual'Ã¨ la peak performance...   \n",
      "2  Ciao serena, in week 9 non mi tornano i dati d...   \n",
      "3  escludere A10S1L1, A10S2L1 (dopo production st...   \n",
      "4  product 7 A1-2-6 considerare solo pdu da 1 a 6...   \n",
      "\n",
      "                  positions pdus weeks              varieties  \n",
      "0                   A13-1-9   16                    product b  \n",
      "1                                                              \n",
      "2                                    9  product 6, product a1  \n",
      "3          A10S1L1, A10S2L1                                    \n",
      "4  A1-2-6, A11-2-2, A11S2L1    1                    product 7  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data for testing\n",
    "SAMPLE_DATA = \"\"\"- 2025-07-18 12:43:08\n",
    "  Username: gianni\n",
    "  Message: Eccoti i dati da escludere questa settimana: A13-1-9 product B escludere pdu16 e 18\n",
    "\n",
    "- 2025-04-07 06:51:35\n",
    "  Username: maria\n",
    "  Message: riusciresti a dirmi qual'Ã¨ la peak performance per referenza? Grazie mille\n",
    "\n",
    "- 2025-03-06 07:29:37\n",
    "  Username: maria\n",
    "  Message: Ciao serena, in week 9 non mi tornano i dati di product 6 e product a1. E' possibile che non sono stati filtrati i dati quando le bilance non erano calibrate?\n",
    "\n",
    "- 2025-02-19 13:33:59\n",
    "  Username: gianni\n",
    "  Message: escludere A10S1L1, A10S2L1 (dopo production start il 3), il resto mi sembra a posto\n",
    "\n",
    "- 2025-06-20 12:34:42\n",
    "  Username: gianni\n",
    "  Message: product 7 A1-2-6 considerare solo pdu da 1 a 6, Misti A11-2-2 considerare solo pdu da 1 a 6, A11S2L1 escludere pdu1-2-3\"\"\"\n",
    "\n",
    "def parse_messages(text):\n",
    "    \"\"\"Parse messages from text\"\"\"\n",
    "    messages = []\n",
    "    pattern = r'- (\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\n\\s+Username: (.*?)\\n\\s+Message: (.*?)(?=\\n- \\d{4}|\\Z)'\n",
    "\n",
    "    for match in re.finditer(pattern, text, re.DOTALL):\n",
    "        messages.append({\n",
    "            'timestamp': match.group(1),\n",
    "            'user': match.group(2),\n",
    "            'message': match.group(3).strip()\n",
    "        })\n",
    "\n",
    "    return messages\n",
    "\n",
    "def analyze_message(msg):\n",
    "    \"\"\"Extract information from a single message\"\"\"\n",
    "    msg_text = msg['message']\n",
    "    msg_lower = msg_text.lower()\n",
    "\n",
    "    # Determine request type\n",
    "    if any(word in msg_lower for word in ['escludere', 'exclude', 'togliere']):\n",
    "        request_type = 'exclusion'\n",
    "    elif any(word in msg_lower for word in ['outcome', 'resa', 'peak', 'performance']):\n",
    "        request_type = 'analysis'\n",
    "    else:\n",
    "        request_type = 'general'\n",
    "\n",
    "    # Extract positions\n",
    "    positions = re.findall(r'A\\d{1,2}[-S]\\d[-L]\\d{1,2}', msg_text)\n",
    "\n",
    "    # Extract PDUs\n",
    "    pdus = re.findall(r'pdu\\s*(\\d+)', msg_text, re.IGNORECASE)\n",
    "\n",
    "    # Extract weeks\n",
    "    weeks = re.findall(r'week\\s*(\\d+)|settimana\\s*(\\d+)', msg_text, re.IGNORECASE)\n",
    "    weeks = [w[0] or w[1] for w in weeks if w[0] or w[1]]\n",
    "\n",
    "    # Extract varieties\n",
    "    varieties = []\n",
    "    variety_list = ['product 6', 'product 7', 'product g2', 'product g3',\n",
    "                    'product m', 'product a1', 'product a2', 'product b']\n",
    "    for v in variety_list:\n",
    "        if v in msg_lower:\n",
    "            varieties.append(v)\n",
    "\n",
    "    return {\n",
    "        'timestamp': msg['timestamp'],\n",
    "        'user': msg['user'],\n",
    "        'request_type': request_type,\n",
    "        'message_preview': msg_text[:80],\n",
    "        'positions': positions,\n",
    "        'pdus': pdus,\n",
    "        'weeks': weeks,\n",
    "        'varieties': varieties\n",
    "    }\n",
    "\n",
    "def generate_simple_sql(analysis):\n",
    "    \"\"\"Generate SQL based on analysis\"\"\"\n",
    "    sql_parts = []\n",
    "\n",
    "    if analysis['request_type'] == 'exclusion':\n",
    "        sql_parts.append(\"-- Exclusion Query\")\n",
    "        sql_parts.append(\"SELECT * FROM outcome_per_ordercode\")\n",
    "        sql_parts.append(\"WHERE test = FALSE\")\n",
    "\n",
    "        for pos in analysis['positions']:\n",
    "            sql_parts.append(f\"  AND position != '{pos}'\")\n",
    "\n",
    "        for pdu in analysis['pdus']:\n",
    "            sql_parts.append(f\"  AND pdu_number != {pdu}\")\n",
    "\n",
    "    elif analysis['request_type'] == 'analysis':\n",
    "        sql_parts.append(\"-- Analysis Query\")\n",
    "        sql_parts.append(\"SELECT variety, week_number,\")\n",
    "        sql_parts.append(\"  AVG(outcome_kg) as avg_outcome,\")\n",
    "        sql_parts.append(\"  MAX(outcome_kg) as peak_outcome\")\n",
    "        sql_parts.append(\"FROM outcome_per_ordercode\")\n",
    "        sql_parts.append(\"WHERE test = FALSE\")\n",
    "\n",
    "        if analysis['varieties']:\n",
    "            variety_list = \"', '\".join(analysis['varieties'])\n",
    "            sql_parts.append(f\"  AND variety IN ('{variety_list}')\")\n",
    "\n",
    "        if analysis['weeks']:\n",
    "            week_list = \", \".join(analysis['weeks'])\n",
    "            sql_parts.append(f\"  AND week_number IN ({week_list})\")\n",
    "\n",
    "        sql_parts.append(\"GROUP BY variety, week_number;\")\n",
    "\n",
    "    else:\n",
    "        sql_parts.append(\"-- General Query\")\n",
    "        sql_parts.append(\"SELECT * FROM outcome_per_ordercode LIMIT 10;\")\n",
    "\n",
    "    return \"\\n\".join(sql_parts)\n",
    "\n",
    "# Main execution\n",
    "print(\"ðŸŒ± VERTICAL FARM MESSAGE ANALYZER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Choose input method\n",
    "print(\"\\n1. Use sample data\")\n",
    "print(\"2. Load from Google Drive\")\n",
    "\n",
    "choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "\n",
    "if choice == '2':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    file_path = input(\"Enter file path (e.g., /content/drive/MyDrive/messages.txt): \")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    except:\n",
    "        print(\"Error loading file, using sample data\")\n",
    "        text = SAMPLE_DATA\n",
    "else:\n",
    "    text = SAMPLE_DATA\n",
    "\n",
    "# Process messages\n",
    "print(\"\\nðŸ“Š Processing messages...\")\n",
    "messages = parse_messages(text)\n",
    "print(f\"Found {len(messages)} messages\")\n",
    "\n",
    "# Analyze each message\n",
    "results = []\n",
    "sql_queries = []\n",
    "\n",
    "for msg in messages:\n",
    "    analysis = analyze_message(msg)\n",
    "    results.append({\n",
    "        'timestamp': analysis['timestamp'],\n",
    "        'user': analysis['user'],\n",
    "        'type': analysis['request_type'],\n",
    "        'message': analysis['message_preview'],\n",
    "        'positions': ', '.join(analysis['positions']),\n",
    "        'pdus': ', '.join(analysis['pdus']),\n",
    "        'weeks': ', '.join(analysis['weeks']),\n",
    "        'varieties': ', '.join(analysis['varieties'])\n",
    "    })\n",
    "\n",
    "    if analysis['request_type'] in ['exclusion', 'analysis']:\n",
    "        sql = generate_simple_sql(analysis)\n",
    "        sql_queries.append(f\"-- {analysis['user']} ({analysis['timestamp']})\\n{sql}\")\n",
    "\n",
    "# Create DataFrame and save\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('analysis_results.csv', index=False)\n",
    "print(f\"\\nâœ… Saved {len(df)} results to analysis_results.csv\")\n",
    "\n",
    "# Save SQL queries\n",
    "if sql_queries:\n",
    "    with open('queries.sql', 'w') as f:\n",
    "        f.write(\"\\n\\n\".join(sql_queries))\n",
    "    print(f\"âœ… Saved {len(sql_queries)} SQL queries to queries.sql\")\n",
    "\n",
    "# Show summary\n",
    "print(\"\\nðŸ“ˆ Summary:\")\n",
    "print(f\"Users: {df['user'].value_counts().to_dict()}\")\n",
    "print(f\"Request types: {df['type'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nâœ… Complete! Check the generated files.\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample results:\")\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
